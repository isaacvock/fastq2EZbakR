# fastq2EZbakR Tips and Tricks

This page is devoted to a number of small pieces of advice and details about running fastq2EZbakR that you may find useful. In particular, information relevant ot the following topics is provided:

1. [fastq2EZbakR runtime](#runtime)
1. [fastq2EZbakR RAM usage](#ram)
1. [Miscellaneous tips](#misc)

## Runtime<a name="runtime"></a>

Rough estimates for runtimes for a single fastq/bam file from human data, with 20 cores provided to Snakemake, are below:

1. 25 million read fastq file: 1-2 hours + time to index genome if necessary (usually ~0.5-1 hours)
1. 300 million read fastq file: 5-7 hours + time to index genome if necessary
1. 25 million read bam file: ~30 minutes
1. 300 million read bam file: 3-4 hours

In most cases, you will likely provide more than a single fastq/bam file as input. If you follow the steps in [Deployment](deploy.md) exactly, then that means most rules will run serially (i.e., not in parallel). Most steps of the pipeline run on a single sample worth of data at a time, so runtimes in this case will increase approximately linearly with the number of input files. **This is often not the best way to run fastq2EZbakR, or any Snakemake pipeline, though!**

More specifically, if you are running fastq2EZbakR in an environment where it is possible to request separate jobs to run in parallel (e.g., an institution's shared HPC clusters that utilize a scheduler like slurm or qsub), then you can significantly improve fastq2EZbakR runtimes by making full use of the computational resources at your disposal. The [Slurm/Yale HPC](slurm.md) section has some details relevant to this, but those instructions are mostly specific to the Yale HPC. I will try and write some more general documentation for this eventually, but the general idea is that you can provide Snakemake with information about your system's job scheduler. When you do this, Snakemake will be able to effectively run multiple input files in parallel through fastq2EZbakR. This has the potential to nearly flatten the number of input files vs. runtime function, dependent on how many jobs your particular system allows you to have running at the same time. Some documentation to check out relevant to this is:

1. If you are using version 8.0 of Snakemake or later, then there are a number of plugins to support optimized execution on a wide array of systems. Checkout the sidebar of this page for the list of plug-ins : https://snakemake.github.io/snakemake-plugin-catalog/
1. If you are using a version of Snakemake older than 8.0, then the same thing is accomplishable via built-in functionality documented here: https://snakemake.readthedocs.io/en/v7.32.3/executing/cluster.html#cluster-generic
1. If you are using version 7.29 of Snakemake or later, I highly suggest checking out the solution described on [John Blischak's relevant repo](https://github.com/jdblischak/smk-simple-slurm), which uses [profiles](https://snakemake.readthedocs.io/en/stable/executing/cli.html#profiles) to implement a simple and elegant solution. Technically, this repo is specific to systems using the slurm scheduler, but the general architecture of the solution can be applied to other systems as well. **NOTE**: A lot changed in version 8.0 of Snakemake, meaning that this repo had to significantly alter the solution to accomodate newer versions of Snakemake. The pre- and post-8.0 solution is available as separate branches on this repo though (e.g., version 7 solution is [here](https://github.com/jdblischak/smk-simple-slurm/tree/v7)).


## RAM usage<a name="ram"></a>

The RAM requirements of fastq2EZbakR are mainly a function of two things:

1. Genome size 
1. Sequencing depth (i.e., number of total reads in your fastq/bam files)

RAM usage is mostly a function of the former. The main RAM bottlenecks are any rules that call STAR, where the size of data structures used by STAR to efficiently accomplish tasks like alignment are a funtion of genome size but **not** fastq file size. These are (`rule name`: description):

1. `align`: Alignment of reads to genome
1. `index`: Indexing a genome for the aligner
1. `maketdf`: Making colored sequencing tracks, which under the hood calls STAR for creating tracks (not alignment; turns out to also be fairly RAM-intensive).

For human data, RAM usage of these steps is typically in the following ballparks:

1. `align`: ~50 GB of RAM
1. `index`: ~120 GB of RAM. **NOTE**: these only need to be built once for a given genome + annotation combination and can be provided to fastq2EZbakR if you already have them.
1. `maketdf`: ~50 GB of RAM for T-to-C mutation content colored tracks

There is one non-STAR step in fastq2EZbakR that can also be fairly RAM-intensive: `merge_features_and_muts`. This rule joins the counts of mutations created by the `cnt_muts` rule with all of the feature assignment tables generated by their relevant rules. The default strategy uses R and the data.table package, loading each feature assignment table into RAM and joining it with the mutation count table. Whereas RAM-usage for the steps listed above is a mostly flat function of genome size, this step's RAM requirement increases as a function of the number of reads in a given fastq/bam file. RAM-usage can thus range from around ~30 GB for a 25 million read library to ~120 GB for a 300 million read library. This RAM-usage scales approximately linearly with the size of the library. If RAM usage for this step is a challenging bottle neck in your case, you can set the `lowRAM` parameter in your config.yaml file to `TRUE`. This implements a strategy to perform the merging without using hardly any RAM. It requires pre-sorting all of the tables to be merged, and then pre-sorting of all of the merged tables prior to combining them into the final cB file. Thus, the downside of this approach is longer runtime. 

On some occasions, the mutation counting step (`cnt_muts`) can also use a considerable amount of RAM. This is only the case if a large number of SNPs are called in the `call_snps` step of the pipeline. SNPs are only called if you provide a non-empty list to the `controls` parameter of the config.yaml, and by extension is only relevant if you have -label data for calling SNPs. Mutation counting is run in parallel via a somewhat naive strategy of splitting up the bam file and using GNU Parallel to run a custom Python script on the separate bam file chunks in parallel. This means that the entire SNP call text file is loaded into RAM in each GNU Parallel job. Thus, RAM usage in this step is roughly equal to the size of the snp.txt file created by fastq2EZbakR * the number of cores provided to this step. 

## Miscellaneous Tips<a name="misc"></a>

Below are a collection of miscellaneous Snakemake/fastq2EZbakR usage tips:

1. If you are using a scheduler to batch jobs in parallel, you will need to set the default amount of RAM requested for each batched jobs. Most steps of fastq2EZbakR require very little RAM, while some require a lot. See discussion of RAM-requirements above. You could thus request the amount of RAM required for the RAM-intensive jobs, but this would mean requesting way more RAM than is necessary for other jobs. A better strategy is to request the amount of RAM necessary for most jobs (which should be around 5-10 GB in most cases), and then specifically request more RAM for RAM-intensive jobs. This can be done by adding `--set-resources <rule name>:<resource name>=<amount> <rule name 2>:<resource name 2>=<amount 2> ...`. `<rule name>` refers to the name of the specific Snakemake "rule" that you are requesting a custom resource for. These are listed when you run Snakemake (or a Snakemake dry run with `snakemake -n`). `<resource name>` is the name of the resource that you want to set a custom amount for. These names are arbitrary and depend on what you call each particular resource in your custom profile or call to Snakemake. This is discussed more [here](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#resources). `<amount>` refers to the custom value you would like to set that resource to for that rule. For example, if you use a Snakemake profile like the one discussed in the Slurm/Yale deployment section of this website (repo [here](https://github.com/isaacvock/yale_profile)), then the amount of RAM requested is given the name "mem_mb". Thus, if I want to request 180 GB of RAM for the alignment index creation step, I would add: `--set-resources index:mem_mb=180000` to my call to Snakemake. You can also request custom job runtimes using this same strategy, which can also be useful in some cases.